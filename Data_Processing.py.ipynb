{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing:\n",
    "\n",
    "This code takes takes out:\n",
    "1. capitalizations\n",
    "2. punctuation characters, \n",
    "3. transforms emoticons to EMOTIC_NEG or EMOTIC_POS.\n",
    "4. corrects words so that instead of goooolll is gol as example.\n",
    "5. Removal of stop words\n",
    "6. Remove URL\n",
    "7. Remove mentions preceeding @ \n",
    "\n",
    "\n",
    "Data contains tweets with values 0=negative,2=neutral and 4=positive.\n",
    "We will be only focusing in binary negative,positive. \n",
    "\n",
    "Citation for data: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-270-21a4a4499eab>, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-270-21a4a4499eab>\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    Make sure they are gramatically correct using textblob\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "######METHODS#######\n",
    "\n",
    "\n",
    "def Process(tweet):\n",
    "    import itertools\n",
    "    import re\n",
    "    from nltk.tokenize import WordPunctTokenizer\n",
    "    from string import punctuation\n",
    "\n",
    "    tok = WordPunctTokenizer()\n",
    "\n",
    "    '''Used to process each tweet to remove capitalizations, gramatical errors and stop words'''\n",
    "    stopwordlist = set(stopwords.words(\"english\"))\n",
    "    d  =enchant.Dict(\"en_US\")\n",
    "    \n",
    "    \n",
    "    #Remove URL links\n",
    "    tweet=re.sub('https?://[A-Za-z0-9./]+','',tweet)\n",
    "    \n",
    "    tweet=tweet.lower()\n",
    "    tweet=tweet.split()\n",
    "    \n",
    "    #Repair gramatically incorrect words \n",
    "    for i in range(len(tweet)):\n",
    "        if d.check(''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet[i]))):\n",
    "            tweet[i]=''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet[i]))\n",
    "        else:\n",
    "            tweet[i]=''.join(''.join(s)[:1] for _, s in itertools.groupby(tweet[i]))\n",
    "    \n",
    "    tweet=' '.join(tweet)\n",
    "    \n",
    "    #Clean using regular expressions\n",
    "    tweet = re.sub(r\"\\'s\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \"ve\", tweet)\n",
    "    tweet = re.sub(r\"n\\'t\", \" nt\", tweet)\n",
    "    tweet = re.sub(r\"\\'re\", \" re\", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" d\", tweet)\n",
    "    tweet = re.sub(r\"\\'ll\", \" ll\", tweet)\n",
    "    tweet = re.sub(r\",\", \"\", tweet)\n",
    "    tweet = re.sub(r\"!\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\(\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\)\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\?\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\s{2,}\", \" \", tweet)\n",
    "    #Remove the @ mention\n",
    "    tweet= re.sub(r'@[A-Za-z0-9]+','',tweet)\n",
    "    \n",
    "    #REmove other punctuations\n",
    "    tweet=\"\".join(c for c in tweet if c not in punctuation)\n",
    "     \n",
    "    \n",
    "    #Remove byte order mark\n",
    "    #test=tweet.decode(\"utf-8-sig\")\n",
    "    #tweet=test.replace(u\"\\ufffd\", \"?\")\n",
    "    \n",
    "    #Remove hashtag\n",
    "    tweet= re.sub(r'#[A-Za-z0-9]+','',tweet)\n",
    "    \n",
    "    #Remove stop words\n",
    "    #liste=[word for word in tweet.split() if word not in stopwordlist]\n",
    "    \n",
    "    # Taking the tweets that have length larger than 1\n",
    "    #liste=[word for word in tweet if len(word)>5]\n",
    "    #tweet=' '.join(liste)   \n",
    "    return tweet.strip().lower()\n",
    "\n",
    "\n",
    "#Make sure they are gramatically correct using textblob\n",
    "#from textblob import TextBlob\n",
    "#ind = [TextBlob(text).word_counts for text in X)]\n",
    "\n",
    "\n",
    "def Process_data(data,label):\n",
    "    sentences=[]\n",
    "    labels=[]\n",
    "    \n",
    "    '''This method does the pre-processing for each data set by iterating over the tweets and applying the Process() method'''\n",
    "    \n",
    "    n,m=data.shape\n",
    "    for i in range(n):\n",
    "        line = data.Tweet[i]\n",
    "        line=Process(line)\n",
    "        sentences.append(line)\n",
    "        if label==True:\n",
    "            labels.append(data.label[i])\n",
    "        \n",
    "        d2=pd.DataFrame(sentences,columns=[\"tweets\"])\n",
    "    if label==True:\n",
    "        d1=pd.DataFrame(labels,columns=[\"label\"])\n",
    "        #convert the labels 0 and 4 to 0-1 for negative and positive sentiment\n",
    "        d1.label.map({0: 0, 4: 1})\n",
    "        Proc=pd.concat([d1, d2], axis=1)\n",
    "    else:\n",
    "        Proc=d2\n",
    "        \n",
    "    \n",
    "    return Proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import enchant\n",
    "\n",
    "#os.cw'/Users/dlebron/Desktop/Twitter_Proj'\n",
    "\n",
    "X=pd.read_csv(\"training.1600000.processed.noemoticon.csv\",encoding = \"ISO-8859-1\",header=None)\n",
    "X.columns=['label','A','B','C','D','Tweet']\n",
    "X=X.drop([\"A\",\"B\",\"C\",\"D\"],axis=1)\n",
    "n,m=X.shape\n",
    "\n",
    "#Subsample 50,000 observations\n",
    "S=50000\n",
    "d1=X[1:int(S/2)]\n",
    "d2=X[-int(S/2):n]\n",
    "\n",
    "X_sub=pd.concat([d1,d2])\n",
    "X_sub=X_sub[X_sub.label != 2]\n",
    "X_sub=X_sub.reset_index()\n",
    "\n",
    "#Load Trump Data\n",
    "\n",
    "Trump = pd.read_excel(\"2017_01_28TrumpTweets.xlsx\")\n",
    " #30,385\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Save the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proc=Process_data(X_sub,label=True)\n",
    "T_proc=Process_data(Trump,label=False)\n",
    "\n",
    "#Check if url's and mentions are removed \n",
    "\n",
    "T_proc.to_csv(\"Trump_Processed.csv\",header=True)\n",
    "X_proc.to_csv(\"Tweets_50kProc.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First day off in over a week... Thought i'd sleep in 4 once, but NO! my body wouldn't let me sleep past 8am  ugh!\""
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sub.Tweet[49880]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first day off in over a week thought i d sleep in 4 once but no my body would nt let me sleep past 8am ugh'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_proc.tweets[49880]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
